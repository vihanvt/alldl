{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8rWzpfItcTR"
      },
      "outputs": [],
      "source": [
        "#import kaggle api\n",
        "!pip install -q kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#importing the dataset\n",
        "!kaggle datasets download -d sumanthvrao/daily-climate-time-series-data\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vopjt8HrpDoy",
        "outputId": "522150fb-4aff-431d-8198-b58dd517c75c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/sumanthvrao/daily-climate-time-series-data\n",
            "License(s): CC0-1.0\n",
            "daily-climate-time-series-data.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip daily-climate-time-series-data\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lnPTwEgpZCL",
        "outputId": "9881b7ad-ca49-4ba9-ff43-66d434c171ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  daily-climate-time-series-data.zip\n",
            "replace DailyDelhiClimateTest.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: DailyDelhiClimateTest.csv  \n",
            "replace DailyDelhiClimateTrain.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: DailyDelhiClimateTrain.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QA4bEs-Nq7au",
        "outputId": "68df8745-9c13-4765-aded-12deefa31b9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "daily-climate-time-series-data.zip  DailyDelhiClimateTrain.csv\tsample_data\n",
            "DailyDelhiClimateTest.csv\t    kaggle.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#we do some preprocessing here\n",
        "import pandas as pd\n",
        "import builtins\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "df=pd.read_csv(\"DailyDelhiClimateTrain.csv\")\n",
        "print(df.head())\n",
        "print(\"-----------------------------------------------------\")\n",
        "print(df.isnull().sum())\n",
        "#wohoo no shitty data thank god\n",
        "df['date']=pd.to_datetime(df['date'])\n",
        "print(\"-----------------------------------------------------\")\n",
        "features=[\"meantemp\",\"humidity\",\"wind_speed\",\"meanpressure\"]\n",
        "df=df[features].values\n",
        "scaler=MinMaxScaler()\n",
        "scaled_train_data=scaler.fit_transform(df)\n",
        "print(scaled_train_data)\n",
        "seq_len=30\n",
        "\n",
        "#-----------------------PREPARING THE DATA TO BE TRAINED ON NOW----------------------------------------------\n",
        "\n",
        "#the rnn is trained on (example btw)- shape(window of 3 elements,their output-i.e the next element)\n",
        "#therefore it learns to predict the next element from the window of seq_len\n",
        "#here the 31st element is the output element for the 1st iteration\n",
        "def seq(scaled_train_data,seq_len):\n",
        "  x_train=[]\n",
        "  y_train=[]\n",
        "  for i in builtins.range(len(scaled_train_data)-seq_len):\n",
        "    x_train.append(scaled_train_data[i:i+seq_len])\n",
        "    y_train.append(scaled_train_data[seq_len+i])\n",
        "  return x_train,y_train\n",
        "x_train,y_train=seq(scaled_train_data,seq_len)\n",
        "x_train=np.array(x_train)\n",
        "y_train=np.array(y_train)\n",
        "print(x_train.shape)#input\n",
        "print(y_train.shape)#output\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GE7YxDgTpppi",
        "outputId": "9c9307a7-74e6-46f1-98e6-505ebb4d4297"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         date   meantemp   humidity  wind_speed  meanpressure\n",
            "0  2013-01-01  10.000000  84.500000    0.000000   1015.666667\n",
            "1  2013-01-02   7.400000  92.000000    2.980000   1017.800000\n",
            "2  2013-01-03   7.166667  87.000000    4.633333   1018.666667\n",
            "3  2013-01-04   8.666667  71.333333    1.233333   1017.166667\n",
            "4  2013-01-05   6.000000  86.833333    3.700000   1016.500000\n",
            "-----------------------------------------------------\n",
            "date            0\n",
            "meantemp        0\n",
            "humidity        0\n",
            "wind_speed      0\n",
            "meanpressure    0\n",
            "dtype: int64\n",
            "-----------------------------------------------------\n",
            "[[0.12227074 0.8209571  0.         0.13260331]\n",
            " [0.04279476 0.90759076 0.07058266 0.132881  ]\n",
            " [0.0356623  0.84983498 0.10974262 0.13299381]\n",
            " ...\n",
            " [0.24745269 0.88063806 0.14842886 0.13289464]\n",
            " [0.276718   0.84983498 0.17349597 0.13265971]\n",
            " [0.12227074 1.         0.         0.1326467 ]]\n",
            "(1432, 30, 4)\n",
            "(1432, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "x9ip4d_UpCSi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "#defining the hyperparamter\n",
        "num_layers=4\n",
        "input_dim=4\n",
        "output_dim=4\n",
        "hidden_dim=128\n",
        "batch_size=32\n",
        "class TimeSeries(nn.Module):\n",
        "    def __init__(self,num_layers,input_dim,output_dim,hidden_dim):\n",
        "        super(TimeSeries,self).__init__()\n",
        "        self.RNN=nn.RNN(input_dim,hidden_dim,num_layers,nonlinearity=\"relu\",bias=True,batch_first=True,dropout=0.0,bidirectional=False,device=None,dtype=None)\n",
        "        self.fc=nn.Linear(hidden_dim,1)\n",
        "    def forward(self,x):\n",
        "        #x_shape=(batch_size,seq_len,input_dim)\n",
        "        out,_=self.RNN(x)\n",
        "        final_out=out[:,-1,:]\n",
        "        output=self.fc(final_out)\n",
        "        return output\n",
        "\n"
      ],
      "metadata": {
        "id": "yWEEuzqAtguo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#preparing test set\n",
        "df_test=pd.read_csv(\"DailyDelhiClimateTest.csv\")\n",
        "print(df_test.head())\n",
        "features=[\"meantemp\",\"humidity\",\"wind_speed\",\"meanpressure\"]\n",
        "df_test=df_test[features].values\n",
        "scaled_test_data=scaler.transform(df)\n",
        "print(scaled_test_data)\n",
        "x_test,y_test=seq(scaled_test_data,seq_len)\n",
        "x_test=np.array(x_test)\n",
        "y_test=np.array(y_test)\n",
        "print(x_test.shape)\n",
        "print(y_test.shape)\n",
        "\n",
        "\n",
        "#splitting into test and validation sets\n",
        "split_ratio=0.8\n",
        "split_index=int(len(x_test)*split_ratio)\n",
        "x_test,x_val=x_test[:split_index],x_test[split_index:]\n",
        "y_test,y_val=y_test[:split_index],y_test[split_index:]\n",
        "\n",
        "x_train=torch.from_numpy(x_train).float()\n",
        "y_train=torch.from_numpy(y_train).float()\n",
        "x_test=torch.from_numpy(x_test).float()\n",
        "y_test=torch.from_numpy(y_test).float()\n",
        "x_val=torch.from_numpy(x_val).float()\n",
        "y_val=torch.from_numpy(y_val).float()\n",
        "\n",
        "#making datasets and dataloaders\n",
        "train_data=torch.utils.data.TensorDataset(x_train,y_train)\n",
        "train_loader=torch.utils.data.DataLoader(train_data,batch_size=batch_size,shuffle=True)\n",
        "val_data=torch.utils.data.TensorDataset(x_val,y_val)\n",
        "val_loader=torch.utils.data.DataLoader(val_data,batch_size=batch_size,shuffle=True)\n",
        "test_data=torch.utils.data.TensorDataset(x_test,y_test)\n",
        "test_loader=torch.utils.data.DataLoader(test_data,batch_size=batch_size,shuffle=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IgE_WopvIfrY",
        "outputId": "75c6768e-c94e-45e2-bc89-fe4a6d04ace4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         date   meantemp   humidity  wind_speed  meanpressure\n",
            "0  2017-01-01  15.913043  85.869565    2.743478     59.000000\n",
            "1  2017-01-02  18.500000  77.222222    2.894444   1018.277778\n",
            "2  2017-01-03  17.111111  81.888889    4.016667   1018.333333\n",
            "3  2017-01-04  18.700000  70.050000    4.545000   1015.700000\n",
            "4  2017-01-05  18.388889  74.944444    3.300000   1014.333333\n",
            "[[0.12227074 0.8209571  0.         0.13260331]\n",
            " [0.04279476 0.90759076 0.07058266 0.132881  ]\n",
            " [0.0356623  0.84983498 0.10974262 0.13299381]\n",
            " ...\n",
            " [0.24745269 0.88063806 0.14842886 0.13289464]\n",
            " [0.276718   0.84983498 0.17349597 0.13265971]\n",
            " [0.12227074 1.         0.         0.1326467 ]]\n",
            "(1432, 30, 4)\n",
            "(1432, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "model = TimeSeries(num_layers, input_dim, output_dim, hidden_dim)\n",
        "epochs = 100\n",
        "lr = 0.001\n",
        "lossfunc = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr)\n",
        "\n",
        "print(\"Training started\")\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for inp, out in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inp)\n",
        "        loss_val = lossfunc(outputs, out)\n",
        "        loss_val.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss_val.item()\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inp, out in val_loader:\n",
        "            outputs = model(inp)\n",
        "            loss_val = lossfunc(outputs, out)\n",
        "            val_loss += loss_val.item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
        "\n",
        "print(\"Training done woops\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phHqYmX19fHU",
        "outputId": "f2bf9838-2fcf-42ec-de71-9cfefde6efd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "Training started\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([16, 4])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([8, 4])) that is different to the input size (torch.Size([8, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([15, 4])) that is different to the input size (torch.Size([15, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Train Loss: 0.077133, Val Loss: 0.075120\n",
            "Epoch [2/100], Train Loss: 0.068684, Val Loss: 0.074710\n",
            "Epoch [3/100], Train Loss: 0.068463, Val Loss: 0.074748\n",
            "Epoch [4/100], Train Loss: 0.068439, Val Loss: 0.074477\n",
            "Epoch [5/100], Train Loss: 0.068539, Val Loss: 0.074580\n",
            "Epoch [6/100], Train Loss: 0.068406, Val Loss: 0.074693\n",
            "Epoch [7/100], Train Loss: 0.068411, Val Loss: 0.074697\n",
            "Epoch [8/100], Train Loss: 0.068375, Val Loss: 0.074467\n",
            "Epoch [9/100], Train Loss: 0.068366, Val Loss: 0.074461\n",
            "Epoch [10/100], Train Loss: 0.068308, Val Loss: 0.074648\n",
            "Epoch [11/100], Train Loss: 0.068477, Val Loss: 0.074632\n",
            "Epoch [12/100], Train Loss: 0.068417, Val Loss: 0.074791\n",
            "Epoch [13/100], Train Loss: 0.068417, Val Loss: 0.074425\n",
            "Epoch [14/100], Train Loss: 0.068256, Val Loss: 0.074420\n",
            "Epoch [15/100], Train Loss: 0.068245, Val Loss: 0.074442\n",
            "Epoch [16/100], Train Loss: 0.068280, Val Loss: 0.074532\n",
            "Epoch [17/100], Train Loss: 0.068303, Val Loss: 0.075025\n",
            "Epoch [18/100], Train Loss: 0.068468, Val Loss: 0.074483\n",
            "Epoch [19/100], Train Loss: 0.068319, Val Loss: 0.074457\n",
            "Epoch [20/100], Train Loss: 0.068268, Val Loss: 0.074460\n",
            "Epoch [21/100], Train Loss: 0.068381, Val Loss: 0.074762\n",
            "Epoch [22/100], Train Loss: 0.068376, Val Loss: 0.074407\n",
            "Epoch [23/100], Train Loss: 0.068262, Val Loss: 0.074474\n",
            "Epoch [24/100], Train Loss: 0.068350, Val Loss: 0.074393\n",
            "Epoch [25/100], Train Loss: 0.068230, Val Loss: 0.074435\n",
            "Epoch [26/100], Train Loss: 0.068300, Val Loss: 0.074507\n",
            "Epoch [27/100], Train Loss: 0.068350, Val Loss: 0.074397\n",
            "Epoch [28/100], Train Loss: 0.068332, Val Loss: 0.074489\n",
            "Epoch [29/100], Train Loss: 0.068388, Val Loss: 0.074456\n",
            "Epoch [30/100], Train Loss: 0.068199, Val Loss: 0.074954\n",
            "Epoch [31/100], Train Loss: 0.068320, Val Loss: 0.074444\n",
            "Epoch [32/100], Train Loss: 0.068264, Val Loss: 0.074565\n",
            "Epoch [33/100], Train Loss: 0.068217, Val Loss: 0.074551\n",
            "Epoch [34/100], Train Loss: 0.068255, Val Loss: 0.074370\n",
            "Epoch [35/100], Train Loss: 0.068208, Val Loss: 0.074338\n",
            "Epoch [36/100], Train Loss: 0.068299, Val Loss: 0.074400\n",
            "Epoch [37/100], Train Loss: 0.068242, Val Loss: 0.074628\n",
            "Epoch [38/100], Train Loss: 0.068336, Val Loss: 0.074403\n",
            "Epoch [39/100], Train Loss: 0.068232, Val Loss: 0.074403\n",
            "Epoch [40/100], Train Loss: 0.068138, Val Loss: 0.074396\n",
            "Epoch [41/100], Train Loss: 0.068158, Val Loss: 0.074616\n",
            "Epoch [42/100], Train Loss: 0.068313, Val Loss: 0.074368\n",
            "Epoch [43/100], Train Loss: 0.068288, Val Loss: 0.074424\n",
            "Epoch [44/100], Train Loss: 0.068336, Val Loss: 0.074366\n",
            "Epoch [45/100], Train Loss: 0.068204, Val Loss: 0.074356\n",
            "Epoch [46/100], Train Loss: 0.068307, Val Loss: 0.074374\n",
            "Epoch [47/100], Train Loss: 0.068260, Val Loss: 0.074427\n",
            "Epoch [48/100], Train Loss: 0.068201, Val Loss: 0.074463\n",
            "Epoch [49/100], Train Loss: 0.068197, Val Loss: 0.074449\n",
            "Epoch [50/100], Train Loss: 0.068136, Val Loss: 0.075761\n",
            "Epoch [51/100], Train Loss: 0.068418, Val Loss: 0.074410\n",
            "Epoch [52/100], Train Loss: 0.068241, Val Loss: 0.074501\n",
            "Epoch [53/100], Train Loss: 0.068252, Val Loss: 0.074407\n",
            "Epoch [54/100], Train Loss: 0.068219, Val Loss: 0.074346\n",
            "Epoch [55/100], Train Loss: 0.068310, Val Loss: 0.074364\n",
            "Epoch [56/100], Train Loss: 0.068176, Val Loss: 0.074422\n",
            "Epoch [57/100], Train Loss: 0.068205, Val Loss: 0.074451\n",
            "Epoch [58/100], Train Loss: 0.068094, Val Loss: 0.074347\n",
            "Epoch [59/100], Train Loss: 0.068206, Val Loss: 0.074325\n",
            "Epoch [60/100], Train Loss: 0.068227, Val Loss: 0.074370\n",
            "Epoch [61/100], Train Loss: 0.068288, Val Loss: 0.074373\n",
            "Epoch [62/100], Train Loss: 0.068281, Val Loss: 0.074517\n",
            "Epoch [63/100], Train Loss: 0.068194, Val Loss: 0.074324\n",
            "Epoch [64/100], Train Loss: 0.068242, Val Loss: 0.074458\n",
            "Epoch [65/100], Train Loss: 0.068196, Val Loss: 0.074622\n",
            "Epoch [66/100], Train Loss: 0.068219, Val Loss: 0.074463\n",
            "Epoch [67/100], Train Loss: 0.068125, Val Loss: 0.074347\n",
            "Epoch [68/100], Train Loss: 0.068190, Val Loss: 0.074465\n",
            "Epoch [69/100], Train Loss: 0.068194, Val Loss: 0.074349\n",
            "Epoch [70/100], Train Loss: 0.068277, Val Loss: 0.074363\n",
            "Epoch [71/100], Train Loss: 0.068142, Val Loss: 0.074401\n",
            "Epoch [72/100], Train Loss: 0.068206, Val Loss: 0.074291\n",
            "Epoch [73/100], Train Loss: 0.068177, Val Loss: 0.074282\n",
            "Epoch [74/100], Train Loss: 0.068170, Val Loss: 0.074348\n",
            "Epoch [75/100], Train Loss: 0.068122, Val Loss: 0.074329\n",
            "Epoch [76/100], Train Loss: 0.068107, Val Loss: 0.074433\n",
            "Epoch [77/100], Train Loss: 0.068123, Val Loss: 0.074362\n",
            "Epoch [78/100], Train Loss: 0.068183, Val Loss: 0.074340\n",
            "Epoch [79/100], Train Loss: 0.068167, Val Loss: 0.074293\n",
            "Epoch [80/100], Train Loss: 0.068162, Val Loss: 0.074302\n",
            "Epoch [81/100], Train Loss: 0.068128, Val Loss: 0.074255\n",
            "Epoch [82/100], Train Loss: 0.068116, Val Loss: 0.074460\n",
            "Epoch [83/100], Train Loss: 0.068158, Val Loss: 0.074263\n",
            "Epoch [84/100], Train Loss: 0.068039, Val Loss: 0.074328\n",
            "Epoch [85/100], Train Loss: 0.068076, Val Loss: 0.074433\n",
            "Epoch [86/100], Train Loss: 0.068055, Val Loss: 0.074241\n",
            "Epoch [87/100], Train Loss: 0.068154, Val Loss: 0.074273\n",
            "Epoch [88/100], Train Loss: 0.068121, Val Loss: 0.074277\n",
            "Epoch [89/100], Train Loss: 0.068106, Val Loss: 0.074432\n",
            "Epoch [90/100], Train Loss: 0.068071, Val Loss: 0.074333\n",
            "Epoch [91/100], Train Loss: 0.068071, Val Loss: 0.074316\n",
            "Epoch [92/100], Train Loss: 0.068200, Val Loss: 0.074291\n",
            "Epoch [93/100], Train Loss: 0.068035, Val Loss: 0.074326\n",
            "Epoch [94/100], Train Loss: 0.068034, Val Loss: 0.074286\n",
            "Epoch [95/100], Train Loss: 0.068116, Val Loss: 0.074268\n",
            "Epoch [96/100], Train Loss: 0.068238, Val Loss: 0.074254\n",
            "Epoch [97/100], Train Loss: 0.068030, Val Loss: 0.074254\n",
            "Epoch [98/100], Train Loss: 0.068063, Val Loss: 0.074205\n",
            "Epoch [99/100], Train Loss: 0.068036, Val Loss: 0.074247\n",
            "Epoch [100/100], Train Loss: 0.068013, Val Loss: 0.074316\n",
            "Training done woops\n"
          ]
        }
      ]
    }
  ]
}
